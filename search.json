[
  {
    "objectID": "posts/2023-03-18-dt-vs-lif/index.html",
    "href": "posts/2023-03-18-dt-vs-lif/index.html",
    "title": "Selection of a timestep for SNN simulation",
    "section": "",
    "text": "Context\nI recently came across a tweet by Dan Goodman, which presented a brief experiment demonstrating the detrimental effects of using a large timestep (\\(\\delta t\\)) during the simulation of a LIF neuron. The output spiking rate of a LIF neuron with a Poisson spike input was found to decrease as the timestep increased, with failure observed at soon as \\(\\delta t=1\\) ms - a standard timestep size within the CS-oriented community.\nOf course, there is a direct relationship between the choice of \\(\\delta t\\), and real-world simulation duration (or wall-clock time). Ideally, we would all be using a very large \\(\\delta t\\) for our simulation. As Guillaume Bellec pointed out, there might not even be any advantage in a machine learning setting to using a small \\(\\delta t\\).\nRather than accepting the necessity of a small timestep, it is worth investigating why the simulation fails, even when employing an exact solver instead of Euler’s method. Specifically, there should only be a small distinction when the spike arrives at the beginning, or the end, of a clock cycle. We somewhat over or underestimate the membrane potential by \\(w\\exp(\\frac{-\\delta t}{\\tau})\\) depending on when the spike arrived during the clock period.\n\n\nRecreating the Simulation\nA straightforward experiment can be devised to replicate the behavior outlined in the tweet. We will simulate 100 LIF neurons, being stimulated by 100 Poisson spike trains sampled at 5 Hz for 4 seconds. The LIF’s time constant is \\(\\tau=10\\) ms. The weights between the 100 inputs and 100 output neurons are randomly sampled from a normal distribution \\(\\mathcal{N}(0.1, 0.25)\\). We then compute the mean output firing rate of every output neuron, and the corresponding standard deviation as error bars.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Configuration\nnp.random.seed(0x1B)\nduration = 4 # seconds\ntau = 0.010\nthresh = 1\nnb_inputs = 100\nnb_outputs = 1000\ninput_rate = 5 #Hz\nweights = np.random.randn(nb_outputs, nb_inputs)*0.5+0.1\ndts = np.logspace(-5, -1.5, 10) # in seconds\n\n# Simulation\nfig, ax = plt.subplots(figsize=(6, 4), tight_layout=True)\nspike_rates = np.zeros((len(dts), nb_outputs)) # output\nfor i, dt in enumerate(dts):\n    time = np.arange(0, duration, dt)\n    u = np.zeros(nb_outputs)\n    _exp = np.exp(-dt/tau)\n    input_spikes = np.random.poisson(lam=input_rate*dt, size=(len(time), nb_inputs))\n    weighted_input_spikes = input_spikes @ weights.T\n    spike_count = 0\n\n    for j, t in enumerate(time):\n        u = _exp * u + weighted_input_spikes[j]\n        spikes = u &gt; thresh\n        spike_count += spikes\n        u[spikes] = 0 # reset\n    spike_rates[i] += spike_count / duration\nax.errorbar(dts*1000, spike_rates.mean(axis=1), yerr=spike_rates.std(axis=1), capsize=5,)\nax.set_xscale(\"log\")\nax.set_xlabel(\"$\\\\delta t$ [ms]\")\nax.set_ylabel(\"Output firing rate [sp/s]\");\n\n\n\n\n\nWe arrive at a similar-looking plot, where the output spiking frequency is going down near \\(\\delta t=1\\) ms.\n\n\nHypothesis\nNumerous commenters in the original thread suggested that \\(\\delta t\\) should be chosen in alignment with \\(\\tau\\). Of course, there is some influence of the chosen time constant \\(\\tau\\), as the smaller the leakage during a timestep, the smaller the error of membrane potential that can happen. However, I am skeptical of this notion due to the stochastic nature of Poisson spikes. Given that a spike can occur at any time during a timestep, it seems likely that the overestimation of membrane potential will roughly cancel out the underestimation. My hypothesis differs from this perspective. I contend that the crucial difference lies elsewhere. Specifically, owing to the nature of the simulation, a neuron can only emit a single spike within a given timestep. Consequently, the LIF neuron enters a sort of implicit refractory period for the duration of the timestep. When the timestep is exceedingly large - greater than 1 ms in this instance - the neuron experiences a prolonged refractory period, leading to the potential loss of critical input spikes as it is unable to integrate new input during this interval.\nIf the assumption is correct, i.e the timestep \\(\\delta t\\) if forcing an implicit refractory period, then having a large refractory period but with a smaller \\(\\delta t\\) should yield the same result as having a larger \\(\\delta t\\). If we add a refractory period to the experiment above, we’ll see that they do indeed provide a similar effect:\n\n\nCode\nfig, ax = plt.subplots(figsize=(6, 4), tight_layout=True)\n\nfor refractory_period in [0.001, 0.01, 0.1]:\n    spike_rates = np.zeros((len(dts), nb_outputs)) # output\n    for i, dt in enumerate(dts):\n        time = np.arange(0, duration, dt)\n        refrac_clk = int(refractory_period/dt)\n        u = np.zeros(nb_outputs)\n        refrac_cntr = np.zeros(nb_outputs, dtype=int)\n        _exp = np.exp(-dt/tau)\n        input_spikes = np.random.poisson(lam=input_rate*dt, size=(len(time), nb_inputs))\n        weighted_input_spikes = input_spikes @ weights.T\n        spike_count = 0\n\n        for j, t in enumerate(time):\n            non_refrac_neurons = refrac_cntr==0\n            u[non_refrac_neurons] = _exp * u[non_refrac_neurons] + weighted_input_spikes[j, non_refrac_neurons]\n            spikes = u &gt; thresh\n            spike_count += spikes\n            u[spikes] = 0 # reset\n\n            # Setup refractory period\n            refrac_cntr = np.maximum(refrac_cntr-1, 0)\n            refrac_cntr[spikes] += refrac_clk\n\n        spike_rates[i] += spike_count / duration\n\n\n    ax.errorbar(dts*1000, spike_rates.mean(axis=1), yerr=spike_rates.std(axis=1), capsize=5, label=f\"Refrac.: {1000*refractory_period:0.1f}ms\")\n    ax.set_xscale(\"log\")\n    ax.set_xlabel(\"$\\\\delta t$ [ms]\")\n    ax.set_ylabel(\"Output firing rate [sp/s]\")\n    ax.legend(loc=\"lower left\")\n\n\n\n\n\nAs we see, the output firing rates align when \\(\\delta t\\) is equal to the refractory period. For example, at \\(\\delta t=10\\) ms, the orange line only starts going down when the timestep becomes bigger than the explicit refractory period. Therefore, the model is actually correct. The only difference is that we have to consider that the effective refractory period is equal to the maximum between \\(\\delta t\\) and the explicit refractory period.\n\n\nSolution\nThe solution to this problem is quite simple. As I said before, the timestep of the simulation forces an implicit refractory period because the neuron can only spike once per timstep. If we remove this limitation, then we should remove this implicit refractory period and the output firing rate should be constant regardless of the timestep.\nTo do so, we count the number of times the membrane potential \\(u(t)\\) is above the threshold to estimate how many times the neuron would spike in one timestep. \\(n_{spikes}(t)=\\lfloor \\frac{\\max \\{u(t), 0\\}}{u_{thresh}} \\rfloor\\). We also edit the reset, such that we remove the threshold from the membrane potential \\(n_{spikes}\\) times, referred to as a soft-reset. This reset mechanism is more precise when dealing with large timesteps, as the accumulated membrane potential is not wasted by an early spike during a timestep. We re-simulate the first experiment with this modification, and we obtain:\n\n\nCode\nnp.random.seed(0x1B)\nfig, ax = plt.subplots(figsize=(6, 4), tight_layout=True)\nspike_rates = np.zeros((len(dts), nb_outputs)) # output\nfor i, dt in enumerate(dts):\n    time = np.arange(0, duration, dt)\n    u = np.zeros(nb_outputs)\n    _exp = np.exp(-dt/tau)\n    input_spikes = np.random.poisson(lam=input_rate*dt, size=(len(time), nb_inputs))\n    weighted_input_spikes = input_spikes @ weights.T\n    spike_count = 0\n\n    for j, t in enumerate(time):\n        u = _exp * u + weighted_input_spikes[j]\n        #previous code: spikes = u &gt; thresh\n        spikes = np.floor(np.maximum(u, 0) / thresh) # multiple spikes\n        spike_count += spikes\n        u -= spikes*thresh\n        #u[spikes &gt; 0] = 0 \n\n    spike_rates[i] += spike_count / duration\nax.errorbar(dts*1000, spike_rates.mean(axis=1), yerr=spike_rates.std(axis=1), capsize=5,)\nax.set_xscale(\"log\")\nax.set_xlabel(\"$\\\\delta t$ [ms]\")\nax.set_ylabel(\"Output firing rate [sp/s]\");\n\n\n\n\n\nAnd voilà! We get the expected firing rate across all the timesteps. While this solution is very interesting for computational neuroscientists, it partly removes the energy friendliness of spiking neural networks since they are not binary anymore, and the reset involves some arithmetic.\n\n\n\n\nCitationBibTeX citation:@online{balafrej2023,\n  author = {Balafrej, Ismael},\n  title = {Selection of a Timestep for {SNN} Simulation},\n  date = {2023-03-19},\n  url = {https://ibalafrej.com/posts/2023-03-18-dt-vs-lif},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBalafrej, Ismael. 2023. “Selection of a Timestep for SNN\nSimulation.” March 19, 2023. https://ibalafrej.com/posts/2023-03-18-dt-vs-lif."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ismael Balafrej",
    "section": "",
    "text": "Hi, I’m Ismael. Welcome to my personal Blog. I currently am doing a PhD in electrical engineering at the University of Sherbrooke, with a thesis in machine learning and neuromorphic engineering. I post content about machine learning, spiking neural networks, data science and more. Feel free to subscribe to the RSS feed."
  },
  {
    "objectID": "index.html#latest-posts",
    "href": "index.html#latest-posts",
    "title": "Ismael Balafrej",
    "section": "Latest posts",
    "text": "Latest posts\n\n\n\n\n\n\n\nPhD thesis defended !\n\n\n4 min\n\n\n\nPhD\n\n\nECG\n\n\n\nLike a proper nerd, I wore an ECG during my thesis defense. Here is the data !\n\n\n\nApr 17, 2023\n\n\n\n\n\n\n\n\n\n\n\nSelection of a timestep for SNN simulation\n\n\n8 min\n\n\n\nSNN\n\n\nLIF\n\n\n\nWhat is the proper timestep to select when simulating a spiking neural network ? The answer is, of course, it depends. Although, I think the usual assumption is incorrect…\n\n\n\nMar 19, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Ismael Balafrej",
    "section": "",
    "text": "PhD thesis defended !\n\n\n4 min\n\n\n\nPhD\n\n\nECG\n\n\n\nLike a proper nerd, I wore an ECG during my thesis defense. Here is the data !\n\n\n\nApr 17, 2023\n\n\n\n\n\n\n\n\n\n\n\nSelection of a timestep for SNN simulation\n\n\n8 min\n\n\n\nSNN\n\n\nLIF\n\n\n\nWhat is the proper timestep to select when simulating a spiking neural network ? The answer is, of course, it depends. Although, I think the usual assumption is incorrect…\n\n\n\nMar 19, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "Here is a summary of my academic publications. You may also find new articles in my Google Scholar  or Semantic Scholar  profiles.\n\nPublications\n\nGoupy, G., Juneau-Fecteau, A., Garg, N., Balafrej, I., Alibart, F., Frechette, L., Drouin, D., Beilliard, Y. Unsupervised and efficient learning in sparsely activated convolutional spiking neural networks enabled by voltage-dependent synaptic plasticity. Neuromorph. Comput. Eng. (2023) https://doi.org/10.1088/2634-4386/acad98\nGarg, N., Balafrej, I., Stewart C. T., Portal, JM. Bocquet, M., Querlioz, D., Drouin, D., Rouat, J., Beillard, Y., Alibart, F. Voltage-Dependent Synaptic Plasticity (VDSP): Unsupervised probabilistic Hebbian plasticity rule based on neurons membrane potential. Frontiers in Neuroscience (2022) https://doi.org/10.3389/fnins.2022.983950.\nBalafrej, I., Alibart, F. & Rouat, J. P-CRITICAL: a reservoir autoregulation plasticity rule for neuromorphic hardware. Neuromorph. Comput. Eng. (2022). https://doi.org/10.1088/2634-4386/ac6533.\nGarg, N., Balafrej, I., Beilliard, Y., Drouin, D., Alibart, F., and Rouat, R. Signals to Spikes for Neuromorphic Regulated Reservoir Computing and EMG Hand Gesture Recognition. International Conference on Neuromorphic Systems 2021, 1–8. ICONS 2021 29. New York, NY, USA: Association for Computing Machinery. https://doi.org/10.1145/3477145.3477267.\nCelotti L., Balafrej, I., Calvet, E. Improving Zero-Shot Neural Architecture Search with Parameters Scoring (2020).\n\n\n\nPresentations\n\nGarg, N., Balafrej, I., Beilliard, Y., Drouin, D., Alibart, F., Rouat. Unsupervised Learning with Ferroelectric Synapses. IEEE 52nd European Solid-State Device Research Conference (2022).\nBalafrej, I., Alibart, F. Rouat, J. Neuromorphic Reservoirs for Energy-Efficient Task-Abstract Machine Learning Devices. Neural Interfaces and Systems Symposium (2021).\nBalafrej, I. Building hardware-friendly models for neuromorphic engineering. Workshop at the University of Montreal (2021).\nGarg, N., Balafrej, I., Beilliard, Y., Drouin, D., Alibart, F., and Rouat, R. Signals to Spikes for Neuromorphic Regulated Reservoir Computing and EMG Hand Gesture Recognition. International Conference on Neuromorphic Systems 2021, 1–8. ICONS 2021 29. New York, NY, USA: Association for Computing Machinery. https://doi.org/10.1145/3477145.3477267.\nBalafrej, I., Rouat, J. P-CRITICAL: A Reservoir Autoregulation Plasticity Rule Designed for Loihi. Intel Neuromorphic Research Community Forum (2020).\nBalafrej, I. Machine learning on neuromorphic processors. 88e Congrès de l’Acfas. (CANCELED - COVID, 2020)\nBalafrej, I. Auto-regulation of Reservoirs with Potential Physiological Signal Monitoring. UNIQUE Symposium (2020)\nBalafrej, I., Rouat, J. Running the CRITICAL Plasticity Rule for Loihi. 3rd Intel Neuromorphic Research Community Workshop, Austria (2019).\nTremblay, J.P., Balafrej, I., Labelle, F., Martel-Denis, F., Matte, É., Chouinard-Beaupré, J., Létourneau, A., Mercier-Nicol, A., Brodeur, S., Ferland, F., Rouat, J. A Cooperative Visually Grounded Dialogue Game with a Humanoid Robot. (2018). Demonstration track, Thirty-third Conference on Neural Information Processing Systems (NeurIPS). Abstract"
  },
  {
    "objectID": "posts/2023-04-13-defense/index.html",
    "href": "posts/2023-04-13-defense/index.html",
    "title": "PhD thesis defended !",
    "section": "",
    "text": "I’m thrilled to announce that I have completed my PhD thesis defense! It’s been a long and challenging journey, but I’m proud of all the hard work and dedication that I’ve put into this moment.\nAlthough I am not an easily stressed person, I thought it would be interesting to capture the intensity of my experience during the defense. To do so, I wore an electrocardiogram during the entire event. In the plot below, the data is presented with arrows indicating the various moments. During the presentation part, my heart rate remained high but relatively stable. As I answered questions from the committee, my heart rate fluctuated, reflecting the varying levels of stress and anxiety that I experienced.\n\n\n\n\n\nHeart rate in beats per minute during my PhD thesis defense on April 13th, 2023.\n\n\n\n\nThis plot does not reflect any meaningful data and only grossly reflects the physical response of my body, I did find it interesting to see all the bumps directly correlated with the various questions. While these bumps could be stress-induced, they may also only reflect the act of speaking with hand movements increasing the necessary blood flow. I enjoyed gathering the extra data, and now I see this plot as a reminder of my success and my ability to overcome challenges in the future. If you’re interested in using the data, is it freely available here (HR) and here (ECG).\nI’m incredibly grateful to my advisors, committee members, friends, and family for their unwavering support throughout this process. Their encouragement and guidance have been invaluable, and I couldn’t have done this without them. Once again, I’m proud to have completed my PhD thesis defense and excited to see where this accomplishment takes me in my future professional endeavors."
  }
]